# Practical Machine Learning Course Project
Author: Anit Bhandari 

```{r, echo=FALSE, message=FALSE}
library(pander)
panderOptions('table.split.table', Inf)
```

## Background
Six young health participants were asked to perform one set of 10 repetitions of the unilateral dumbbell biceps curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). These activities were monitored with on-body wearable accelerometers.

The task that I intend to preform is to develop a model that predicts what activity is being preformed based on the data that was collected, and use this model to classify twenty cases.  I will accomplish this by selecting the model that predicts with the highest degree of accuracy.

## Feature Selection
I begin the process by loading the libraries I will need throughout the analysis.  As I want my research to be reproducible I will set the random number generation seed to 7:
```{r, message=FALSE}
library(caret)
library(rattle)
set.seed(7)
```

Next I download the data if it doesn't exist in the working directory:
```{r}
training.file <- 'pml-training.csv'
if(!file.exists(training.file)){
  download.file('http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', training.file)
}
testing.file <- 'pml-testing.csv'
if(!file.exists(testing.file)){
  download.file('http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', testing.file)
}
```

The data has missing values and excel error messages.  I cleaned these up and standardized the missing values as I load the data into the environment:
```{r}
training <- read.csv(training.file, na.strings=c('NA','#DIV/0!',''))
testing <- read.csv(testing.file, na.strings=c('NA','#DIV/0!',''))
```

Initially there are `r ncol(training)` features.  This is too many to create a useful prediction model so I determined I needed to remove some of the features.  I removed all features that have are nothing but missing values:
```{r}
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
```

After this step I am left with `r ncol(training)` features.  Next I deleted the first 7 features (`r names(training)[1:7]`) as they are meta data and not useful in forming a prediction model:
```{r}
training <- training[, -c(1:7)]
testing <- testing[, -c(1:7)]
```

I am now left with a pool of `r ncol(training)` possible features for inclusion in the prediction model.  A list of the feature and a sample of the data is given in the appendix.

## Partitioning Data Set

The data set that I am going to create my prediction model from contains `r nrow(training)` observations.  In order to cross validate the models I partitioned this data set into two subsets.  I used 60% of the data for model training and 40% for cross validation.
```{r}
subset <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
subset.for.training <- training[subset,]
subset.for.testing <- training[-subset,]
```

## Modelling Approaches

### Recursive Partitioning and Regression Trees
I selected this approach as a way to form a baseline.  I did not expect this to provide a reliable prediction model.  However I needed something to compare the other models against.
```{r, cache=TRUE, message=FALSE}
rpart.fit <- train(classe ~ ., data=subset.for.training, method='rpart')
predict.rpart <- predict(rpart.fit, subset.for.testing)
fancyRpartPlot(rpart.fit$finalModel)
rpart.cm <- confusionMatrix(predict.rpart, subset.for.testing$classe)
rpart.cm
```

```{r, echo=FALSE, message=FALSE}
## Free Up RAM
rm(rpart.fit)
```


### Random Forest
I selected this approach out of curiosity to see if it could out preform the Bayesian model.
```{r, cache=TRUE, message=FALSE}
rf.fit <- train(classe ~ ., data = subset.for.training, method = 'rf', 
                trControl = trainControl(method = "cv", number = 4, allowParallel = TRUE), importance=TRUE)
predict.rf <- predict(rf.fit, subset.for.testing)
rf.cm <- confusionMatrix(predict.rf, subset.for.testing$classe)
rm.imp <- varImp(rf.fit)
rf.cm
```

## Model Evaluation
```{r, echo=FALSE}
plot.data <- data.frame('Model'=character(), 'Accuracy'=numeric(), stringsAsFactors=FALSE)
plot.data[1,1] <- 'RPART'
plot.data[1,2] <- rpart.cm$overall[1]
plot.data[2,1] <- 'Random Forest'
plot.data[2,2] <- rf.cm$overall[1]
#plot.data$Model <- as.factor(plot.data$Model)
ggplot(data=plot.data, aes(x=Model, y=Accuracy, fill=Model)) + geom_bar(stat='identity') + guides(fill=FALSE) + ggtitle('Prediction Model Accuracy Comparison')
```

Because of the high level of accuracy of the random forest model, I chose to use it to predict classify the 20 test cases.

## Out of Sample Error
The out of sample accuracy rate for the random forest model is `r rf.cm$overall[1]*100`%.  Consequently the out of sample error rate is 1 - `r rf.cm$overall[1]` or `r (1-rf.cm$overall[1])*100`%.

## Write Files
The final step is to write out the predictions for submission:  
```{r, eval=FALSE, message=FALSE}
# Prepare files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predict(rf.fit, testing))
```

```{r, echo=FALSE, message=FALSE}
## Free Up RAM
rm(rf.fit)
```

# Appendix 1: Features  
```{r, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
pander(names(training), style = 'rmarkdown')
```

# Appendix 2: Sample Data  
```{r table-simple, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
pander(head(training), style = 'rmarkdown')
```